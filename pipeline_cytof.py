"""===========================
Pipeline template
===========================

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_cytofkit.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
import rpy2.robjects as ro
import CGAT.IOTools as IOTools
import glob

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_genesets.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database_name"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


# ---------------------------------------------------
# Specific pipeline task
def run_tsne_generator():
    '''Group FCS files by sample, supports n replicates'''
    
    files = glob.glob("data.dir/*FCS")

    groups = []
    for f in files:
        group = os.path.basename(f).split("_")[0]
        if group in groups:
            pass
        else:
            groups.append(group)

    group_dict = {}
    for f in files:
        for group in groups:
            if group in f:
                if group in group_dict:
                    group_dict[group].append(f)
                else:
                    group_dict[group] = [f]

    for key in group_dict.keys():
        infiles = group_dict[key]
        outfile = os.path.basename(key) + ".matrix.touch"

        yield([infiles, outfile])
        
   
@files(run_tsne_generator)
def run_tsne(infiles, outfile):
    '''Process FCS data and run tSNE. Supports multiple options for 
       tsne parameters "perplexity" and "iterations" '''

    job_memory = "40G"
    pipeline_dir = PARAMS["pipeline_dir"]
    
    infiles = ','.join(infiles)

    marker_list = PARAMS["analysis_markers"] # markers to use for tsne & downstream analysis
    perplexity = str(PARAMS["tsne_perplexity"]).split(",")
    iterations = str(PARAMS["tsne_iterations"]).split(",")
    events = PARAMS["analysis_no_events"] # no events per sample
    
    statements = []

    for p in perplexity:
        for i in iterations:

            out_norm = outfile.replace(".matrix.touch", "") + "." + str(p) + "_" + str(i) + ".norm.RData"
            out_tsne = outfile.replace(".matrix.touch", "") + "." + str(p) + "_" + str(i) + ".tsne.RData"
            log = outfile.replace(".touch", ".log")
        
            statements.append('''Rscript %(pipeline_dir)s/R/cytofkit.R 
                                   --infiles %(infiles)s 
                                   --out_norm %(out_norm)s 
                                   --out_tsne %(out_tsne)s
                                   --markers %(marker_list)s                                    
                                   --perplexity %(p)s 
                                   --iterations %(i)s
                                   --no_events %(events)s
                                   &> %(log)s''' % locals() )

    print(statements)

    P.run()

    IOTools.touchFile(outfile) # creates sentinel file for task monitoring

   
@follows(run_tsne)
@transform("*.tsne.RData",
           regex(r"(.*).tsne.RData"),
           r"\1.cluster.touch")
def run_clustering(infile, outfile):
    '''Run clustering algorithms on tSNE dim reduced data'''

    job_memory = "20G"
    pipeline_dir = PARAMS["pipeline_dir"]
    
    clust_methods = PARAMS["analysis_clust_methods"].split(",")
    # Rphenograph, ClusterX, FlowSOM
    
    statements = []
    
    for clust in clust_methods:
        outname = outfile.replace(".cluster.touch", ".") + str(clust) + ".RData"
        log = outfile.replace(".cluster.touch", ".") + str(clust) + ".log"

        if clust == "Rphenograph": 
            infile = infile.replace(".tsne.RData", ".norm.RData") # runs on high dimensionality data
            no_clust = " "
            print("Rphenograph")
            
        elif clust == "FlowSOM":
            infile = infile.replace(".tsne.RData", ".norm.RData") # runs on high dimensionality data
            no_clust = "--noClusters " + str(PARAMS["analysis_clust_no"])
            print("FlowSOM")
            
        elif clust == "ClusterX":
            infile = infile # run on tSNE reduced data
            no_clust = " "
            print("ClusterX")
            
        else:
            print("Specify clustering method in pipeline.ini")

            
        statements.append('''Rscript %(pipeline_dir)s/R/CYTOFclust.R
                               --infile %(infile)s
                               --outfile %(outname)s
                               --clusterMethod %(clust)s
                               %(no_clust)s
                               &> %(log)s''' % locals() )
        
    print(statements)
    
    P.run()

    IOTools.touchFile(outfile)


@follows(run_clustering)
@transform("*.tsne.RData",
           regex(r"(.*).tsne.RData"),
           r"\1.merged.RData")
def mergeCYTOFdata(infile, outfile):
    '''merge normalised data, tsne, and clusters'''

    job_memory = "5G"
    pipeline_dir = PARAMS["pipeline_dir"]
    
    # files w/ RData suffix: norm, tsne, ClusterX, Rphenograph, FlowSOM
    sample = infile.replace(".tsne.RData", "")
    
    clusterx = glob.glob(sample + ".ClusterX.RData")
    if clusterx:
        clusterx = clusterx[0]
        opt_clusterx = '''--ClusterX %(clusterx)s'''
    else:
        opt_clusterx='''--ClusterX "none" '''
        
    phenograph = glob.glob(sample + ".Rphenograph.RData")
    if phenograph:
        phenograph = phenograph[0]
        opt_phenograph = '''--Rphenograph %(phenograph)s'''
    else:
        opt_phenograph='''--Rphenograph "none" '''
        
    flowsom = glob.glob(sample + ".FlowSOM.RData")
    if flowsom:
        flowsom = flowsom[0]
        opt_flowsom = '''--FlowSOM %(flowsom)s'''
    else:
        opt_flowsom='''--FlowSOM "none" '''

    statement = '''Rscript %(pipeline_dir)s/R/mergeData.R
                     --infile %(infile)s
                     --outfile %(outfile)s
                     %(opt_clusterx)s
                     %(opt_phenograph)s
                     %(opt_flowsom)s''' % locals()

    print(statement)
    
    P.run()
    
    
# ---------------------------------------------------
# Generic pipeline tasks
@follows(mergeCYTOFdata)
def full():
    pass


def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))    
