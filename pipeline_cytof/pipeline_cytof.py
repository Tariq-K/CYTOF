"""===========================
Pipeline template
===========================

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_cytofkit.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGAT.Database as DB
import CGATPipelines.Pipeline as P
import rpy2.robjects as ro
import CGAT.IOTools as IOTools
import glob
import pandas as pd
from functools import reduce

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_genesets.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database_name"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


# ---------------------------------------------------
# Specific pipeline task

# Currently data is processed & normalised using cytofkit & tSNE is run on that data in 1 step
# this creates duplicates of processed data for each tSNE run. This is not neccessary as most clustering
# methods use high dimensional data. tSNE is just for visualisation.

# TODO
# 1) Split data processing & normalisation into a seperate task
# 2) Run tNSE on processed data and set a "default" tSNE setting which can be altered in pipeline.ini,
#    or tweaked after the pipeline has run

def Rgenerator():
    '''Group FCS files by sample, supports n replicates'''
    
    files = glob.glob("data.dir/*FCS")

    groups = []
    for f in files:
        group = os.path.basename(f).split("_")[0]
        if group in groups:
            pass
        else:
            groups.append(group)

    group_dict = {}
    for f in files:
        for group in groups:
            if group in f:
                if group in group_dict:
                    group_dict[group].append(f)
                else:
                    group_dict[group] = [f]

    for key in group_dict.keys():
        infiles = group_dict[key]
        outfile = "data.dir/" + os.path.basename(key) + ".norm.txt"

        yield([infiles, outfile])

@files(Rgenerator)
def normaliseFCS(infiles, outfile):
    '''Normalise FCS data with cytofkit. Subset data to panel specified in pipeline.ini'''

    infiles = ','.join(infiles)
    pipeline_dir = PARAMS["pipeline_dir"]

    marker_list = PARAMS["analysis_panel"] # list of all markers in panel
    events = PARAMS["analysis_no_events"] # no events per sample
    transform = PARAMS["analysis_transform"]
    
    statement = '''Rscript %(pipeline_dir)sR/cytofkit_new.R 
                    --infiles %(infiles)s
                    --outfile %(outfile)s
                    --dataTransform %(transform)s
                    --markers %(marker_list)s
                    --no_events %(events)s'''
    print(statement)

    P.run()                  


@transform(normaliseFCS,
           suffix(r".txt"),
           r".load")
def loadNormaliseFCS(infile, outfile):

    tablename = os.path.basename(outfile).replace(".load", "").replace(".", "_")

    cols = 'id,' + PARAMS["analysis_panel"]
    options='-H "%(cols)s" ' % locals()

    statement = '''tail -n +2 %(infile)s | ''' + P.build_load_statement(tablename, options=options, retry=True) + ''' > %(outfile)s'''

    print(statement)
    
    to_cluster = True

    P.run() 
    
@follows(normaliseFCS, mkdir("tSNE.dir"))
@transform(normaliseFCS,
           regex(r"data.dir/(.*).norm.txt"),
           r"tSNE.dir/\1.tsne.touch")
def runTsne(infile, outfile):
    '''Run tSNE. Supports multiple options for 
       tsne parameters "perplexity" and "iterations" '''

    job_memory = "40G"
    pipeline_dir = PARAMS["pipeline_dir"]
    
    marker_list = PARAMS["tsne_markers"] # markers to use for tsne & downstream analysis
    perplexity = str(PARAMS["tsne_perplexity"]).split(",")
    iterations = str(PARAMS["tsne_iterations"]).split(",")
    
    statements = []

    for p in perplexity:
        for i in iterations:
            out_tsne = outfile.replace(".tsne.touch", "") + "." + str(p) + "_" + str(i) + ".tsne.txt"
            log = outfile.replace(".touch", ".log")
        
            statements.append('''Rscript %(pipeline_dir)sR/tsne.R 
                                   --infile %(infile)s 
                                   --outfile %(out_tsne)s
                                   --markers %(marker_list)s                                    
                                   --perplexity %(p)s 
                                   --iterations %(i)s
                                   &>> %(log)s''' % locals() )

    print(statements)

    P.run()

    IOTools.touchFile(outfile) # creates sentinel file for task monitoring


@follows(runTsne)
@transform("tSNE.dir/*.tsne.txt",
           suffix(r".txt"),
           r".load")
def loadRunTsne(infile, outfile):
    
    P.load(infile, outfile)
    
    
@follows(normaliseFCS, mkdir("clustering.dir"))
@transform(normaliseFCS,
           regex(r"data.dir/(.*).norm.txt"),
           r"clustering.dir/\1.cluster.touch")
def run_clustering(infile, outfile):
    '''Run clustering algorithms on tSNE dim reduced data'''

    job_memory = "20G"
    pipeline_dir = PARAMS["pipeline_dir"]
    
#    clust_methods = PARAMS["clustering_methods"].split(",")
# Phenograph only, but vary value of k (no. neighbours). Lower k = more clusters

    statements = []
    
    opts = [ "--k " + x for x in PARAMS["clustering_k"].split(',') ]
    # k values from 20+ work well
    
    clust = "phenograph"
    marker_list = PARAMS["clustering_markers"] # markers to use for tsne & downstream analysis
    
    for opt in opts:
        if len(opt) > 1:
            outname = outfile.replace(".cluster.touch", ".") + str(clust) + "_" + opt.split(' ')[-1] + ".txt"
            log = outfile.replace(".cluster.touch", ".") + str(clust)  + "_" + opt.split(' ')[-1] + ".log"
        else:
            outname = outfile.replace(".cluster.touch", ".") + str(clust) + ".txt"
            log = outfile.replace(".cluster.touch", ".") + str(clust) + ".log"


        statements.append('''Rscript %(pipeline_dir)sR/CYTOFclust.R
                               --infile %(infile)s
                               --markers %(marker_list)s                                    
                               --outfile %(outname)s
                               %(opt)s
                               &> %(log)s''' % locals() )

    print(statements)
    
    P.run()

    IOTools.touchFile(outfile)

    
@transform(run_clustering,
           regex(r"(.*).cluster.touch"),
           r"\1.phenograph.txt")
def mergePhenograph(infile, outfile):
    '''merge all phenograph runs into table for upload'''

    files = glob.glob(infile.replace(".cluster.touch", ".phenograph_*.txt"))

    n = 0
    for f in files:
        n = n +1
        if n == 1:
            res = pd.read_csv(f, sep="\t")
        else:
            df = pd.read_csv(f, sep="\t")
            res = pd.merge(res, df, how="inner", left_index=True, right_index=True)

    res["id"] = res.index.values
    res.to_csv(outfile, sep="\t", header=True, index=None)

    
@transform(mergePhenograph,
           suffix(r".txt"),
           r".load")
def loadMergePhenograph(infile, outfile):

    P.load(infile, outfile)

@follows(loadMergePhenograph, mkdir("results.dir"))
@transform("tSNE.dir/*.txt",
           regex(r"tSNE.dir/(.*)\.([0-9]*)_([0-9]*).tsne.txt"),
           add_inputs(r"data.dir/\1.norm.load", r"clustering.dir/\1.phenograph.txt"),
           r"results.dir/\1.\2_\3_tsne.merged.txt")
def mergeCYTOFresults(infiles, outfile):

    db = PARAMS["database"]
    table = os.path.basename(infiles[1]).replace(".load", "").replace(".", "_")
    query = '''select * from %(table)s''' % locals()

    fcs = DB.fetch_DataFrame(query, db)
    tsne = pd.read_csv(infiles[0], sep="\t")
    clust = pd.read_csv(infiles[2], sep="\t")

    df = reduce(lambda left, right: pd.merge(left, right, how="inner", on="id"), [tsne, fcs, clust])

    df.to_csv(outfile, sep="\t", index=False)

@transform(mergeCYTOFresults,
           suffix(".txt"),
           r".load")
def loadMergeCYTOFresults(infile, outfile):

#    P.load(infile, outfile)

    tablename = os.path.basename(outfile).replace(".load", "").replace(".", "_")

    statement = '''cat %(infile)s | ''' + P.build_load_statement(tablename, retry=True) + ''' > %(outfile)s'''

    print(statement)
    
    to_cluster = True

    P.run() 
          
    
@follows(run_clustering, mkdir("results.dir"))
@transform("tSNE.dir/*.tsne.RData",
           regex(r"tSNE.dir/(.*).tsne.RData"),
           r"results.dir/\1.merged.RData")
def mergeCYTOFdata(infile, outfile):
    '''merge normalised data, tsne, and clusters'''

    job_memory = "5G"
    pipeline_dir = PARAMS["pipeline_dir"]
    
    # files w/ RData suffix: norm, tsne, ClusterX, Rphenograph, FlowSOM
    sample = "clustering.dir/" + os.path.basename(infile).split(".")[0]
    print(sample)
    clusterx = glob.glob(sample + ".ClusterX.RData")
    if clusterx:
        clusterx = clusterx[0]
        opt_clusterx = '''--ClusterX %(clusterx)s'''
    else:
        opt_clusterx='''--ClusterX "none" '''
        
    phenograph = glob.glob(sample + ".Rphenograph.RData")
    if phenograph:
        phenograph = phenograph[0]
        opt_phenograph = '''--Rphenograph %(phenograph)s'''
    else:
        opt_phenograph='''--Rphenograph "none" '''
        
    flowsom = glob.glob(sample + ".FlowSOM.RData")
    if flowsom:
        flowsom = flowsom[0]
        opt_flowsom = '''--FlowSOM %(flowsom)s'''
    else:
        opt_flowsom='''--FlowSOM "none" '''


    statement = '''Rscript %(pipeline_dir)sR/mergeData.R
                     --infile %(infile)s
                     --outfile %(outfile)s
                     %(opt_phenograph)s
                ''' % locals()
    
    ### No longer support clusterX and flowSOM algorithms, phenograph is better
    
                     # %(opt_clusterx)s
                     # %(opt_flowsom)s

    print(statement)
    
    P.run()
    
    
# ---------------------------------------------------
# Generic pipeline tasks
@follows(loadNormaliseFCS, loadRunTsne, loadMergeCYTOFresults)
def full():
    pass


def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))    
